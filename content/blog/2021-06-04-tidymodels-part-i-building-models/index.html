---
title: 'Tidymodels Part I: Building models'
author: Ji Huang
date: '2021-06-04'
output: 
    html_document:
        toc: true
        code_folding: show
categories:
  - dry lab
tags:
  - statistics
  - R
meta_img: images/image.png
description: Description for the page

---

<script src="index_files/header-attrs/header-attrs.js"></script>


<p>TOC:</p>
<ol start="0" style="list-style-type: decimal">
<li><p>Prep</p></li>
<li><p>Build the model without workflow</p></li>
<li><p>Build the model with workflow</p></li>
<li><p>Add resampling</p></li>
<li><p>Add hyperparameter tunning</p></li>
</ol>
<p>Inspired by the Julia Silge’s <a href="https://www.tidyverse.org/blog/2021/05/choose-tidymodels-adventure/">blog</a>. I want to try the tidymodels system by add blocks to the backbone. To keep it simple, I used <code>ranger</code> engine for a simple randome forest model on the <a href="https://www.rdocumentation.org/packages/modeldata/versions/0.1.0/topics/cells">cells data</a> (comes with tidymodels).</p>
<div id="prep" class="section level2">
<h2>0. Prep</h2>
<pre class="r"><code>library(tidymodels)

tidymodels_prefer()

## define a function for lollipop plot. also called Cleveland dot plots.
## https://www.r-graph-gallery.com/303-lollipop-plot-with-2-values.html
plot_lollipop &lt;- function(tain_metrics, test_metrics) {
    
    bind_rows(tain_metrics, test_metrics) %&gt;% 
        select(-.estimator) %&gt;% 
        pivot_wider(names_from = type, values_from = .estimate) %&gt;% 
        ggplot()+
        geom_segment(aes(x= .metric, xend= .metric, 
                         y=test_set, yend=train_set), color=&quot;grey&quot;) +
        geom_point(aes(x= .metric, y = test_set, color=&quot;red&quot;), size=3)+
        ggrepel::geom_text_repel(aes(x= .metric, y = test_set, 
                                     label = round(test_set,2)))+
        geom_point(aes(x= .metric, y = train_set, color = &quot;blue&quot;), size=3 ) +
        ggrepel::geom_text_repel(aes(x= .metric, y = train_set,
                                     label = round(train_set,2)))+
        scale_color_manual(values=c(&quot;blue&quot;, &quot;red&quot;), label = c(&quot;training&quot;, &quot;testing&quot;))+
        #coord_flip()+
        labs(x = &quot;&quot;, y =&quot;&quot;, color = &quot;Dataset&quot;)+
        cowplot::theme_cowplot()

}</code></pre>
<p>Load dataset.</p>
<pre class="r"><code>data(&quot;cells&quot;)
cells &lt;- cells %&gt;% select(-case) # `case` is author defined training, testing set.</code></pre>
<p>Setting the <code>seed</code> for the <code>ranger()</code> is the key for reproducibility!</p>
</div>
<div id="build-the-model-without-workflow" class="section level2">
<h2>1. Build the model without workflow</h2>
<p>In this part, I did not use <code>workflow</code>. The idea is: split dataset –&gt; train the model on the training set –&gt; get the performance on training set –&gt; fit the same model on testing set and get the performance on testing set.</p>
<p>1.1 Split dataset.</p>
<pre class="r"><code>set.seed(123)
data_split &lt;- initial_split(cells, strata = class)
data_train &lt;- training(data_split)
data_test &lt;- testing(data_split)</code></pre>
<p>1.2 Train model on training set.</p>
<pre class="r"><code>ranger_model &lt;- rand_forest() %&gt;%
    set_mode(&quot;classification&quot;) %&gt;%
    set_engine(&quot;ranger&quot;, seed = 12)

ranger_fit &lt;- ranger_model %&gt;% 
    fit(class ~ ., data = data_train)</code></pre>
<p>1.3 Check model performance on training set.</p>
<pre class="r"><code>rf_training_pred &lt;- 
    predict(ranger_fit, data_train) %&gt;% 
    bind_cols(predict(ranger_fit, data_train, type = &quot;prob&quot;)) %&gt;% 
    bind_cols(data_train %&gt;% select(class))

rf_train_metrics &lt;- rf_training_pred %&gt;%   # training set predictions
    roc_auc(truth = class, .pred_PS) %&gt;% 
    bind_rows(
        rf_training_pred %&gt;%
    accuracy(truth = class, .pred_class)
    ) %&gt;% 
    mutate(type = &quot;train_set&quot;)</code></pre>
<p>1.4 Fit the model on testing and check performance.</p>
<pre class="r"><code>rf_testing_pred &lt;- 
    predict(ranger_fit, data_test) %&gt;% 
    bind_cols(predict(ranger_fit, data_test, type = &quot;prob&quot;)) %&gt;% 
    bind_cols(data_test %&gt;% select(class)) 

rf_test_metrics &lt;- rf_testing_pred %&gt;% 
    roc_auc(truth = class, .pred_PS) %&gt;% 
    bind_rows(
        rf_testing_pred %&gt;% 
            accuracy(truth = class, .pred_class)
    ) %&gt;% 
    mutate(type = &quot;test_set&quot;)</code></pre>
<p>Plot AUROC and Accuracy measurements on training and testing set.</p>
<pre class="r"><code>plot_lollipop(rf_train_metrics, rf_test_metrics)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="build-the-model-with-workflow" class="section level2">
<h2>2. Build the model with workflow</h2>
<p>In the second part, I added a little block called <code>workflow</code>. It combines the <em>pre-processing</em> and <em>model</em> step together.</p>
<p>2.1 Split dataset. Same as the first part.</p>
<pre class="r fold-hide"><code>set.seed(123)
data_split &lt;- initial_split(cells, strata = class)
data_train &lt;- training(data_split)
data_test &lt;- testing(data_split)</code></pre>
<p>2.2 Create model workflow</p>
<pre class="r"><code>ranger_recipe &lt;- 
    recipe(formula = class ~ ., data = data_train) 

ranger_spec &lt;-
    rand_forest() %&gt;%
    set_mode(&quot;classification&quot;) %&gt;%
    set_engine(&quot;ranger&quot;, seed = 12)

ranger_workflow &lt;- workflow() %&gt;% 
    add_recipe(ranger_recipe) %&gt;% 
    add_model(ranger_spec)</code></pre>
<p>2.3 Train the workflow on training set and get performance.</p>
<p>Mostly same as part 1, but this time fit the <em>workflow</em>.</p>
<pre class="r fold-hide"><code>ranger_fit &lt;- fit(ranger_workflow, data = data_train)

rf_training_pred &lt;- 
    predict(ranger_fit, data_train) %&gt;% 
    bind_cols(predict(ranger_fit, data_train, type = &quot;prob&quot;)) %&gt;% 
    bind_cols(data_train %&gt;% select(class))

rf_train_metrics &lt;- rf_training_pred %&gt;%
    roc_auc(truth = class, .pred_PS) %&gt;% 
    bind_rows(
        rf_training_pred %&gt;%
    accuracy(truth = class, .pred_class)
    ) %&gt;% 
    mutate(type = &quot;train_set&quot;)</code></pre>
<p>2.4 Fit the workflow on testing set and check performance.</p>
<p>Same as part I.</p>
<pre class="r fold-hide"><code>rf_testing_pred &lt;- 
    predict(ranger_fit, data_test) %&gt;% 
    bind_cols(predict(ranger_fit, data_test, type = &quot;prob&quot;)) %&gt;% 
    bind_cols(data_test %&gt;% select(class)) 

rf_test_metrics &lt;- rf_testing_pred %&gt;% 
    roc_auc(truth = class, .pred_PS) %&gt;% 
    bind_rows(
        rf_testing_pred %&gt;% 
            accuracy(truth = class, .pred_class)
    ) %&gt;% 
    mutate(type = &quot;test_set&quot;)</code></pre>
<pre class="r"><code>plot_lollipop(rf_train_metrics, rf_test_metrics)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
<div id="add-resampling" class="section level2">
<h2>3. Add resampling</h2>
<p>In the third part, add resampling to the training process. This time I used 10 fold Cross Validation (CV). The purpose is to get a better measurement of the training model performance. It generated 10 models, but we do not use any of these models. When we predict, the model is trained using the whole training set and then is used to predict the testing set.</p>
<p>Many of the step is the same</p>
<p>3.1 Split dataset</p>
<pre class="r fold-hide"><code>set.seed(123)
data_split &lt;- initial_split(cells, strata = class)
data_train &lt;- training(data_split)
data_test &lt;- testing(data_split)</code></pre>
<p>3.2 Create CV set</p>
<p>You can set <code>repeat = 5</code> to repeat the CV process for even better performance measurement.</p>
<pre class="r"><code>set.seed(234)
data_folds &lt;- vfold_cv(data_train, strata = class, repeats = 1)
head(data_folds)</code></pre>
<pre><code>## # A tibble: 6 x 2
##   splits             id    
##   &lt;list&gt;             &lt;chr&gt; 
## 1 &lt;split [1362/152]&gt; Fold01
## 2 &lt;split [1362/152]&gt; Fold02
## 3 &lt;split [1362/152]&gt; Fold03
## 4 &lt;split [1362/152]&gt; Fold04
## 5 &lt;split [1362/152]&gt; Fold05
## 6 &lt;split [1363/151]&gt; Fold06</code></pre>
<p>3.3 Create model workflow</p>
<pre class="r fold-hide"><code>ranger_recipe &lt;- 
    recipe(formula = class ~ ., data = data_train) 

ranger_spec &lt;-
    rand_forest() %&gt;%
    set_mode(&quot;classification&quot;) %&gt;%
    set_engine(&quot;ranger&quot;, seed = 13)

ranger_workflow &lt;- workflow() %&gt;% 
    add_recipe(ranger_recipe) %&gt;% 
    add_model(ranger_spec)</code></pre>
<p>3.4 Train the model on the CV training set and get performance</p>
<pre class="r"><code>doParallel::registerDoParallel()
set.seed(12345)
ranger_rs &lt;-
    fit_resamples(ranger_workflow,
                  resamples = data_folds,
                  control = control_resamples(save_pred = TRUE)
    )</code></pre>
<p>Summarize the 10-fold CV.</p>
<pre class="r"><code>rf_train_metrics &lt;- collect_metrics(ranger_rs, summarize = TRUE)
rf_train_metrics</code></pre>
<pre><code>## # A tibble: 2 x 6
##   .metric  .estimator  mean     n std_err .config             
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary     0.834    10 0.00924 Preprocessor1_Model1
## 2 roc_auc  binary     0.903    10 0.00694 Preprocessor1_Model1</code></pre>
<pre class="r"><code>## Process the metrics for plotting
rf_train_metrics &lt;- rf_train_metrics %&gt;% rename(.estimate = mean) %&gt;% 
    select(-std_err, -.config, -n) %&gt;% 
    mutate(type = &quot;train_set&quot;)</code></pre>
<p>Performance on each fold.</p>
<pre class="r"><code>head(collect_metrics(ranger_rs, summarize = FALSE))</code></pre>
<pre><code>## # A tibble: 6 x 5
##   id     .metric  .estimator .estimate .config             
##   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 Fold01 accuracy binary         0.882 Preprocessor1_Model1
## 2 Fold01 roc_auc  binary         0.918 Preprocessor1_Model1
## 3 Fold02 accuracy binary         0.842 Preprocessor1_Model1
## 4 Fold02 roc_auc  binary         0.932 Preprocessor1_Model1
## 5 Fold03 accuracy binary         0.816 Preprocessor1_Model1
## 6 Fold03 roc_auc  binary         0.895 Preprocessor1_Model1</code></pre>
<p>Plot the AUC plot.</p>
<pre class="r"><code>collect_predictions(ranger_rs) %&gt;%
    group_by(id) %&gt;%
    roc_curve(class, .pred_PS) %&gt;%
    autoplot()</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>Plot the confusion matrix.</p>
<pre class="r"><code>conf_mat_resampled(ranger_rs, tidy = FALSE) %&gt;%
    autoplot()</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>3.5 Fit the model on the testing set and get performance</p>
<pre class="r"><code>ranger_fit &lt;- fit(ranger_workflow, data = data_train)

rf_testing_pred &lt;- 
    predict(ranger_fit, data_test) %&gt;% 
    bind_cols(predict(ranger_fit, data_test, type = &quot;prob&quot;)) %&gt;% 
    bind_cols(data_test %&gt;% select(class)) 

rf_test_metrics &lt;- rf_testing_pred %&gt;% 
    roc_auc(truth = class, .pred_PS) %&gt;% 
    bind_rows(
        rf_testing_pred %&gt;% 
            accuracy(truth = class, .pred_class)
    ) %&gt;% 
    mutate(type = &quot;test_set&quot;)</code></pre>
<p>We can see the difference between training and testing is much smaller now!</p>
<pre class="r"><code>plot_lollipop(rf_train_metrics, rf_test_metrics)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-22-1.png" width="672" />
Another way to fit the model on the testing set is to use <code>last_fit()</code></p>
<pre class="r"><code>final_fitted &lt;- last_fit(ranger_workflow, data_split)
collect_metrics(final_fitted)</code></pre>
<pre><code>## # A tibble: 2 x 4
##   .metric  .estimator .estimate .config             
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary         0.814 Preprocessor1_Model1
## 2 roc_auc  binary         0.892 Preprocessor1_Model1</code></pre>
<p>As long as you set the <code>seed</code> parameter for <code>ranger()</code>, then the results should be exactly the same. You can check all prediction values are the same</p>
<pre class="r"><code>sum(rf_testing_pred$.pred_PS != final_fitted$.predictions[[1]]$.pred_PS)</code></pre>
<pre><code>## [1] 0</code></pre>
</div>
<div id="add-hyperparameter-tunning" class="section level2">
<h2>4. Add hyperparameter tunning</h2>
<p>In the last part, I added a tuning step.</p>
<pre class="r"><code>library(vip)</code></pre>
<p>4.1 Split data and create CV set</p>
<pre class="r fold-hide"><code>set.seed(123)
data_split &lt;- initial_split(cells, strata = class)
data_train &lt;- training(data_split)
data_test &lt;- testing(data_split)

set.seed(234)
data_folds &lt;- vfold_cv(data_train, strata = class, repeats = 1)
head(data_folds)</code></pre>
<pre><code>## # A tibble: 6 x 2
##   splits             id    
##   &lt;list&gt;             &lt;chr&gt; 
## 1 &lt;split [1362/152]&gt; Fold01
## 2 &lt;split [1362/152]&gt; Fold02
## 3 &lt;split [1362/152]&gt; Fold03
## 4 &lt;split [1362/152]&gt; Fold04
## 5 &lt;split [1362/152]&gt; Fold05
## 6 &lt;split [1363/151]&gt; Fold06</code></pre>
<p>4.2 Create workflow with tunning</p>
<p>Include the <code>importance = "impurity"</code>, so we can get feature importance score latter.</p>
<pre class="r"><code>ranger_recipe &lt;- 
    recipe(formula = class ~ ., data = data_train) 

ranger_spec &lt;-
    rand_forest(mtry = tune(), min_n = tune()) %&gt;%
    set_mode(&quot;classification&quot;) %&gt;%
    set_engine(&quot;ranger&quot;, seed = 13, importance = &quot;impurity&quot;)

ranger_workflow &lt;- workflow() %&gt;% 
    add_recipe(ranger_recipe) %&gt;% 
    add_model(ranger_spec)

ranger_param &lt;- parameters(ranger_spec) %&gt;% 
    update(mtry = mtry(c(3, 20)),
           min_n = min_n(c(2, 40)))

# Use grid search.
ranger_grid &lt;- grid_regular(ranger_param, levels = c(mtry = 3, min_n = 2))

ranger_grid</code></pre>
<pre><code>## # A tibble: 6 x 2
##    mtry min_n
##   &lt;int&gt; &lt;int&gt;
## 1     3     2
## 2    11     2
## 3    20     2
## 4     3    40
## 5    11    40
## 6    20    40</code></pre>
<p>4.3 Tune the hyperparameters.</p>
<pre class="r"><code>doParallel::registerDoParallel()
set.seed(12345)
ranger_tune &lt;-
    tune_grid(ranger_workflow, 
              resamples = data_folds, 
              grid = ranger_grid)</code></pre>
<p>Check performance</p>
<pre class="r"><code>head(ranger_tune %&gt;% collect_metrics())</code></pre>
<pre><code>## # A tibble: 6 x 8
##    mtry min_n .metric  .estimator  mean     n std_err .config             
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               
## 1     3     2 accuracy binary     0.830    10 0.00926 Preprocessor1_Model1
## 2     3     2 roc_auc  binary     0.904    10 0.00694 Preprocessor1_Model1
## 3    11     2 accuracy binary     0.829    10 0.00958 Preprocessor1_Model2
## 4    11     2 roc_auc  binary     0.904    10 0.00713 Preprocessor1_Model2
## 5    20     2 accuracy binary     0.828    10 0.00737 Preprocessor1_Model3
## 6    20     2 roc_auc  binary     0.902    10 0.00663 Preprocessor1_Model3</code></pre>
<pre class="r"><code>ranger_tune %&gt;%
    collect_metrics() %&gt;%
    mutate(mtry = factor(mtry)) %&gt;%
    ggplot(aes(min_n, mean, color = mtry)) +
    geom_line(size = 1.5, alpha = 0.6) +
    geom_point(size = 2) +
    facet_wrap(~ .metric, scales = &quot;free&quot;, nrow = 2) +
    scale_color_viridis_d(option = &quot;viridis&quot;, begin = .9, end = 0)+
    theme_bw()</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>Get the best model</p>
<pre class="r"><code>ranger_tune %&gt;% show_best(&quot;roc_auc&quot;)</code></pre>
<pre><code>## # A tibble: 5 x 8
##    mtry min_n .metric .estimator  mean     n std_err .config             
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               
## 1     3     2 roc_auc binary     0.904    10 0.00694 Preprocessor1_Model1
## 2    11     2 roc_auc binary     0.904    10 0.00713 Preprocessor1_Model2
## 3    11    40 roc_auc binary     0.903    10 0.00779 Preprocessor1_Model5
## 4    20     2 roc_auc binary     0.902    10 0.00663 Preprocessor1_Model3
## 5     3    40 roc_auc binary     0.902    10 0.00763 Preprocessor1_Model4</code></pre>
<pre class="r"><code>best_model &lt;- ranger_tune %&gt;%
    select_best(&quot;roc_auc&quot;)

best_model</code></pre>
<pre><code>## # A tibble: 1 x 3
##    mtry min_n .config             
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;               
## 1     3     2 Preprocessor1_Model1</code></pre>
<p>4.4 Finalize the model/workflow and fit the training set</p>
<pre class="r"><code>final_wf &lt;- 
    ranger_workflow %&gt;% 
    finalize_workflow(best_model)

final_wf</code></pre>
<pre><code>## == Workflow ====================================================================
## Preprocessor: Recipe
## Model: rand_forest()
## 
## -- Preprocessor ----------------------------------------------------------------
## 0 Recipe Steps
## 
## -- Model -----------------------------------------------------------------------
## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   mtry = 3
##   min_n = 2
## 
## Engine-Specific Arguments:
##   seed = 13
##   importance = impurity
## 
## Computational engine: ranger</code></pre>
<p>Fit training set</p>
<pre class="r"><code>final_model &lt;- final_wf %&gt;% fit(data_train)
final_model</code></pre>
<pre><code>## == Workflow [trained] ==========================================================
## Preprocessor: Recipe
## Model: rand_forest()
## 
## -- Preprocessor ----------------------------------------------------------------
## 0 Recipe Steps
## 
## -- Model -----------------------------------------------------------------------
## Ranger result
## 
## Call:
##  ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~3L,      x), min.node.size = min_rows(~2L, x), seed = ~13, importance = ~&quot;impurity&quot;,      num.threads = 1, verbose = FALSE, probability = TRUE) 
## 
## Type:                             Probability estimation 
## Number of trees:                  500 
## Sample size:                      1514 
## Number of independent variables:  56 
## Mtry:                             3 
## Target node size:                 2 
## Variable importance mode:         impurity 
## Splitrule:                        gini 
## OOB prediction error (Brier s.):  0.1199526</code></pre>
<p>4.5 Explore feature importance</p>
<pre class="r"><code>final_model %&gt;% 
    pull_workflow_fit() %&gt;% 
    vip() + theme_bw()</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>4.6 Fit testing set and check performance</p>
<pre class="r"><code>final_fit &lt;- 
    final_wf %&gt;%
    last_fit(data_split) </code></pre>
<pre class="r"><code>final_fit %&gt;%
    collect_metrics()</code></pre>
<pre><code>## # A tibble: 2 x 4
##   .metric  .estimator .estimate .config             
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary         0.814 Preprocessor1_Model1
## 2 roc_auc  binary         0.888 Preprocessor1_Model1</code></pre>
<pre class="r"><code>final_fit %&gt;%
    collect_predictions() %&gt;% 
    roc_curve(class, .pred_PS) %&gt;% 
    autoplot()</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<pre class="r"><code>final_fit %&gt;%
    collect_predictions() %&gt;% 
    pr_curve(class, .pred_PS) %&gt;% 
    autoplot()</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-36-2.png" width="672" /></p>
</div>
